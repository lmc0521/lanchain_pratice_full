{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004e9b34-2618-4c4d-a0fc-7277fd36b523",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MermaidDrawMethod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 293\u001b[0m\n\u001b[0;32m    287\u001b[0m custom_graph \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))\u001b[39;00m\n\u001b[0;32m    290\u001b[0m display(\n\u001b[0;32m    291\u001b[0m     Image(\n\u001b[0;32m    292\u001b[0m         custom_graph\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_mermaid_png(\n\u001b[1;32m--> 293\u001b[0m             draw_method\u001b[38;5;241m=\u001b[39m\u001b[43mMermaidDrawMethod\u001b[49m\u001b[38;5;241m.\u001b[39mAPI,\n\u001b[0;32m    294\u001b[0m         )\n\u001b[0;32m    295\u001b[0m     )\n\u001b[0;32m    296\u001b[0m )\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m#Run\u001b[39;00m\n\u001b[0;32m    300\u001b[0m inputs\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplan how the different type of agent memory work?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m    303\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MermaidDrawMethod' is not defined"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings, OllamaEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import Chroma, FAISS, SKLearnVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "local_llm=\"llama3\"\n",
    "\n",
    "#Load\n",
    "url=\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "loader=WebBaseLoader(url)\n",
    "docs=loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# vectordb=FAISS.from_documents(docs,embedding)\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    You will be given: \\n\n",
    "    1/ a question\n",
    "    2/ a retrieved document    \n",
    "\n",
    "    question: {question} \\n\n",
    "    retrieved document: \\n\\n {documents} \\n\\n\n",
    "    \n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "'''\n",
    "prompt= PromptTemplate(\n",
    "    templete=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    Here is the retrieved document: \\n\\n {documents} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "'''\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "# question = \"Explan how the different type of agent memory work?\"\n",
    "# question=\"Explain how AlphaCodium works?\"\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[0].page_content\n",
    "# print(retrieval_grader.invoke({\"question\": question, \"documents\": doc_txt}))\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "    Use the following documents to answer the question. \n",
    "\n",
    "    If you don't know the answer, just say that you don't know. \n",
    "\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Documents: {documents} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "# generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "# print(generation)\n",
    "\n",
    "### Search\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    search: str\n",
    "    documents: List[str]\n",
    "    steps: List[str]\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print('---RETRIEVE---')\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"retrieve_documents\")\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print('---GENERATE---')\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"generate_answer\")\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print('---CHECK RELEVENCE---')\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"grade_document_retrieval\")\n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    points = 0\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"documents\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        print(grade)\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print('---GRADDE: DOCUMENT RELEVANT---')\n",
    "            filtered_docs.append(d)\n",
    "            points+=1\n",
    "        else:\n",
    "            print('---GRADDE: DOCUMENT NOT RELEVANT---')\n",
    "            # search = \"Yes\"\n",
    "            # continue\n",
    "            points-=1\n",
    "    print(points)\n",
    "    if points<0:\n",
    "        search=\"Yes\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"search\": search,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    print('---WEB SEARCH---')\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"web_search\")\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    documents.extend(\n",
    "        [\n",
    "            Document(page_content=d[\"content\"], metadata={\"url\": d[\"url\"]})\n",
    "            for d in web_results\n",
    "        ]\n",
    "    )\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print('---DECIDE TO GENERATE---')\n",
    "    search = state[\"search\"]\n",
    "    if search == \"Yes\":\n",
    "        print('---DECISION: RUN WEB SEARCH---')\n",
    "        return \"search\"\n",
    "    else:\n",
    "        print('---DECISION: GENERATE---')\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "custom_graph = workflow.compile()\n",
    "\n",
    "# display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "display(\n",
    "    Image(\n",
    "        custom_graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "#Run\n",
    "\n",
    "inputs={\n",
    "        \"question\":\"Explan how the different type of agent memory work?\",\n",
    "        \"steps\": []\n",
    "}\n",
    "# inputs={\n",
    "#         \"question\":\"what is agent memory?\",\n",
    "#         \"steps\": []\n",
    "# }\n",
    "# inputs={\n",
    "#         \"question\":\"Explain how AlphaCodium works?\",\n",
    "#         \"steps\": []\n",
    "# }\n",
    "\n",
    "for output in custom_graph.stream(inputs):\n",
    "    for key,value in output.items():\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "pprint.pprint(value['generation'])\n",
    "\n",
    "\n",
    "'''\n",
    "def predict_custom_agent_local_answer(example: dict):\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    state_dict = custom_graph.invoke(\n",
    "        {\"question\": example[\"input\"], \"steps\": []}, config\n",
    "    )\n",
    "    return {\"response\": state_dict[\"generation\"], \"steps\": state_dict[\"steps\"]}\n",
    "\n",
    "\n",
    "example = {\"input\": \"What are the types of agent memory?\"}\n",
    "response = predict_custom_agent_local_answer(example)\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdd28f-8ac6-4d0a-b171-8bdfe762b4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
